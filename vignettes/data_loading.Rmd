---
title: "Data loading methods for the R package glatos"
date: "Updated: `r Sys.Date()`"
output: 
  rmarkdown::pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Data loading methods for the R package glatos}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

#set 'str' options to desired output format
str_opts <- getOption("str") #get list of options
str_opts$strict.width = "wrap"
str_opts$vec.len = 1
options(str = str_opts)

#set 'width'
options(width = 85)

```

\pagebreak

# Overview

This vignette describes methods for loading data into the R package 
*glatos*. Sections are organized by data type (*detections*, 
*receiver locations*, etc), and each section contains examples for:

1. data in standardized formats from the Great Lakes Acoustic Telemetry 
Observation System (GLATOS), the Ocean Tracking Network (OTN), and VEMCO using 
built-in data loading functions; and

2. data in non-standard formats that require loading using non-*glatos* 
functions and modification to meet *glatos* requirements.


## Loading data from GLATOS, OTN, and VEMCO

The *glatos* package contains five functions (see [Built-in
functions](#built-in-functions)) designed to load data files in standardized 
formats from the GLATOS, the OTN, and VEMCO. Each data loading function:  

1. loads data into an R session consistently and efficiently using the best
available methods and  

2. returns an object that meets the requirements of *glatos* package functions.  

Thus, using *glatos* load functions ensures that resulting data conform to the
requirements of other functions in the package (e.g., *summarize_detections*,
*detection_bubble_plot*) and relieves users from the work of reformatting their
data to meet requirements of each specific function.


### Built-in functions

The *glatos* package includes five data loading functions:
    
- _**read_glatos_detections**_  
 : reads detection data from a comma-separated-values 
         text file obtained from the GLATOS Data Portal and returns an object of class 
          *glatos_detections* that is also a *data.frame*. 
  
- _**read_otn_detections**_
 : reads detection data from a comma-separated-values 
      text file obtained from the Ocean Tracking Network and returns an object of 
      class *glatos_detections* that is also a *data.frame*.
  
- _**read_glatos_receivers**_
 : reads receiver data from a comma-separated-values 
      text file obtained from the GLATOS Data Portal and returns an object of 
      class *glatos_receivers* that is also a *data.frame*.
  
- _**read_glatos_workbook**_
 : reads data from a GLATOS project-specific 
      MS Excel workbook (\*.xlsm file) and returns a list of class 
      *glatos_workbook* with two-elements; one of class *glatos_receivers* and 
      one of class *glatos_animals* (both are also *data.frame*s).
  
- _**read_vemco_tag_specs**_
 : reads tag specification data from an MS Excel 
      workbook (\*.xls file) provided by VEMCO and returns a list with two 
      elements; one containing tag specifications and one containing tag 
      operating schedules. (both are also *data.frame*s).
  
### Data objects and classes

Most of the functions listed above return an object with a *glatos*-specific S3
class name (e.g., *glatos_detections*) in addition to a more general class
(e.g., *data.frame*). Currently, no methods exist for *glatos* classes and such
classes are not explicitly required by any function, so *glatos* classes can
merely be thought of as labels showing that the objects were produced by a
*glatos* function and will therefore be compatible with other *glatos*
functions. Beware, as with any S3 class, that it is possible to modify a
*glatos* object to the point that is will no longer be compatible with *glatos*
functions. The [Data Requirements vignette](data_requirements.html) provides 
an overview of data requirements of *glatos* functions.

## Loading data from other sources

To use *glatos* functions with data that are not in one of the standard formats 
described above, those data will need to be:  

1. loaded into R using some other function (e.g., *read_csv*) and 

2. modified to ensure that all requirements of the desired function are met. 

Strictly speaking, there are no requirements of the package as a whole, but 
input data are checked within each individual function to determine if 
requirements are met. Nonetheless, the 
[Data Requirements vignette](data_requirements.html) provides a set of data 
requirements, including column names, types, and formats, that will ensure 
compatibility with all *glatos* functions. 

For each data type (e.g., *detection*, *receiver location*, etc), this 
vignette shows how data from a comma-separated-values text file can be 
loaded into R using non-*glatos* functions and then modified to meet the 
*glatos* requirements.


## Tips to improve speed and efficiency

The main examples in the this vignette use only base R functions. However, 
there are many contributed packages that can provide functions that can improve 
workflow speed and efficiency. After most examples using base R 
functions, boxed examples are also given to expose users to alternative 
examples using functions from contributed packages. Most boxed tips 
show use of functions from the *data.table* package. If you are not familiar with *data.table*, then read through the introductory 
vignette (see `vignette("datatable-intro", package = "data.table")`). For more on *data.table*, see the vignettes (`browseVignettes("data.table")`). In addition 
to *data.table*, one boxed tip draws from the *lubridate* package because it 
is, to our knowledge, the fastest way to coerce timestamps strings to the 
date-time class *POSIXct*. 
No other examples show use of *tidyverse* packages (*dplyr*, *ggplot2*, 
*readr*), simply because some of us have not yet drunk the kool-aid. 
A future version of this vignette may include more examples using the 
*tidyverse* or other packages.

A few notes about boxed-example code:  

1. Make sure the relevant packages are installed and attached:  

> ```{r results = "hide", warning = FALSE, message = FALSE}
> #install.packages("data.table")
> library(data.table)
> 
> #install.packages("lubridate")
> library(lubridate)
> ```

2. For *data.table* functions, make sure you've converted the target object 
to *data.table* class using *setDT*:

> ```{r results = "hide", warning = FALSE, message = FALSE}
> dx <- data.frame(a = 1 , b = 2)
> setDT(dx) #convert to data.table
> ```

3. Go all-in; don't run some base R examples and some boxed example or it 
will cause trouble. For example if base R example changes column names in 
an object, then the old column names won't exist if you later run the 
boxed example that changes those same column names using *data.table* 
functions.  




# Detection data 

## Requirements

*glatos* functions that accept detection data as input will typically require a 
*data.frame*.  Please refer to "Data field definitions of standard
GLATOS detection export" vignette and function help (e.g.,
`?read_glatos_detections`) to identify data type and definition of
mandatory fields (link).

mandatory fields:
- detection\_timestamp\_utc (link)
- receiver\_sn (link)
- deploy\_lat (link)
- deploy\_long (link)
- transmitter\_codespace (link)
- transmitter\_id (link)
- sensor\_value (link)
- sensor\_unit (link)
- animal\_id (link)

 Additionally, some functions will require at least one categorical column to 
 identify location (or group of locations). These can be specified by the user, 
 but examples of such columns in a GLATOS standard detection file are:
 
- glatos\_array (link)
- station (link)
- glatos\_project\_receiver (link)

Any *data.frame* that contains the above columns should be compatible with 
all *glatos* functions that accept detection data as input. Use of the 
data loading functions *read\_glatos\_detections* and *read\_otn\_detections* 
will ensure that these columns are present, but can only be used on data 
in GLATOS and OTN formats. Data in other formats will need to be loaded 
using other functions (e.g., *read.csv()*, *data.table::fread()*, etc.) 
and compatibility with *glatos* functions will need to be carefully checked.

## Examples 

### Loading GLATOS data

The *read_glatos_detections()* function reads in detection data from
standard detection exports (\*.csv files) obtained from the GLATOS
standard data export and checks that the data meet requirements of
*glatos* functions. Data are read using *fread* in the *data.table*
package, timestamps are formatted as class *POSIXct* and dates are
formatted as class *Date*.

First, we will use *system.file()* to get the path to the walleye_detections.csv 
file included in the *glatos* package.

```{r echo=TRUE}
# Set path to walleye_detections.csv example dataset
wal_det_file <- system.file('extdata', 'walleye_detections.csv', package = 'glatos')
```


Next, we will load data from *walleye_detection.csv* using 
*read_glatos_detections()* and view the structure of the resulting 
data frame.

```{r echo=TRUE}
# Attach glatos package to global environment.
library(glatos)

# Read in the walleye_detections.csv file using `read_glatos_detections()`
walleye_detections <- read_glatos_detections(wal_det_file)

# View the structure and data from first row
str(walleye_detections)
```

The result is an object with 30 columns (including the columns
described above) and two classes: *glatos_detections* and
*data.frame*. Please refer to "Data field definitions of standard
GLATOS detection export" vignette for field definitions (link).  The
*glatos_detections* class label indicates that the data set was
created using a glatos load function and therefore should work with
any *glatos* function that accepts detection data as input.


### Loading OTN data 

The *read_otn_detections()* function reads in detection data (\*.csv files) 
obtained from the Ocean Tracking Network and reformats the data to meet
requirements of *glatos* functions. Data are read using 
*fread* in the *data.table* package, timestamps are 
formatted as class *POSIXct* and dates are formatted as class *Date*.

```{r echo=TRUE}
# Read in the blue_shark_detections.csv file using `read_glatos_detections()`
shrk_det_file <- system.file("extdata", "blue_shark_detections.csv",
                         package = "glatos")

# Read in the blue_shark_detections.csv file using `read_otn_detections()`
blue_shark_detections <- read_otn_detections(shrk_det_file)


# View the structure of blue_shark_detections
str(blue_shark_detections)
```

The result shares the same classes as the walleye dataset
(*glatos_detections* and *data.frame*), but has 34 columns, most of
which are not shared between OTN and GLATOS networks. Thus,
*read_otn_detections* has only altered those OTN-specific columns
needed to meet requirements of *glatos* functions.  Field definitions
and data types can be found in "Data field definitions of standard
GLATOS detection export" vignette (link).

### Other formats - CSV file exported from a VUE database

Detection data in any format than GLATOS or OTN will need to be  
modified to meet the requirements of *glatos* functions. We will show an 
example using detection data that have been exported from a VEMCO VUE database. 
There is currently no *glatos* function to load detection data directly into 
an R session from VUE software, so data in that format will need to be loaded 
using other functions and then carefully checked that it meets requirements 
described above. 

In the example below, we will use the base R functions *read.csv()* and 
*as.POSIXct()* to load detection data from a csv file and reformat the data to 
be consistent with the schema described above. Tip boxes will also show 
faster alternatives for these methods using functions in the *data.table* and 
*lubridate* packages.

First, get the path to a file (\*.csv) that
contains detection data exported from VEMCO VUE software. Such a file
is not included in the *glatos* package but we can create one using the 
*glatos* function *vrl2csv()* which exports detection
data from a VEMCO \*.vrl file using VUE's built-in command line
conversion program. The *vrl2csv()* function requires a working copy
of Vemco VUE on your computer. VUE can be downloaded from  https://vemco.com/products/vue-software. 

```{r, echo=TRUE}
#get path to example VRL included with glatos package
vrl_file <- system.file("extdata", "VR2W_109924_20110718_1.vrl",
                        package = "glatos")

#convert the vrl file to csv
#note that vrl2csv writes the csv to disk and returns the path to the csv
csv_file <- vrl2csv(vrl_file) #file name input
```

Now that we have the path to a VUE export file, we will read the data using 
*read.csv()*. In this case we are also setting some *read.csv()* arguments 
to non-default values. First, we set `as.is = TRUE` so that character values
are treated as characters and not converted to factors. Second, we 
set `check.names = FALSE` to prevent conversion of syntactically-invalid 
column names to syntactically valid names. This simply keeps the names 
exactly as they appear in the source text file rather than, for example, 
replacing spaces with ".". This does mean that we need to wrap those column 
names in back-ticks when called (e.g., ``my_det$`Sensor Value` ``). Third, 
we set `fileEncoding = "UTF-8-BOM"` to match the encoding of the text file. 
If this argument is omitted then you might see the special characters `ï»¿`
added to the first column name. Setting the *fileEncoding* may also slow 
down the import.

```{r}
dtc <- read.csv(csv_file, as.is = TRUE, check.names = FALSE,  
                fileEncoding = "UTF-8-BOM")

```

> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip:__ Use _fread_ instead of _read.csv_. 

> ``` {r eval = FALSE}  
> #read data from csv file using data.table::fread
> dtcc <- fread(csv_file)  
> ```  
> 
> _fread_ is fast. That's one reason it is used used by _read_glatos_detections_ and other _glatos_ functions.  
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


Now we will reformat to be consistent with a *glatos_detections* object. We 
will do this for each of the mandatory columns described above.

#### _**detection_timestamp_utc**_

Change the column name from *Date and Time (UTC)* to *detection\_timestamp\_utc* 
and format as *POSIXct*. We could reference columns by number in the code 
below (e.g., `names(dtc)[1] <- "detection_timestamp_utc"`) but use of 
`match()` to get the column number is robust to changes in column order.

```{r}
#change column name
names(dtc)[match("Date and Time (UTC)", names(dtc))] <- "detection_timestamp_utc"
```


> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip:__ Use _setnames_ to change column names.  

> ``` {r eval = FALSE} 
> #use data.table::setnames to change column names via old and new names
> setnames(dtc, "Date and Time (UTC)", "detection_timestamp_utc") 
> ```  
> 
> Notice that there is no assignment operator (_<-_) in this code. This is  
> because _setnames_, like other _data.table_ functions, updates the target 
> object (in this case _dtc_) directly (aka: "by reference").  
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


We will format the timestamp column using base R 
function *as.POSIXct()*. All *POSIXct* objects are 
stored internally as a number representing the number of elapsed seconds since 
"1970-01-01 00:00:00" in UTC. When we convert a character string to *POSIXct*, 
we need to tell R *how* to convert it--namely the time zone of the input data. 
By default, *as.POSIXct* will assume your local system time zone (e.g., the one 
returned by `Sys.timezone()`. To prevent timezone errors, *always* 
specify time zone (using the *tz* argument) whenever you coerce any 
timestamp to *POSIXct*. In this case, the timestamps were exported from VUE in 
UTC, so we use the following:

```{r}

dtc$detection_timestamp_utc <- as.POSIXct(dtc$detection_timestamp_utc,
                                                  tz = "UTC")

str(dtc$detection_timestamp_utc)
```

> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip:__ Use _:=_ to add or modify a column.  

> _:=_ is an assignment operator for _data.table_ objects that assigns objects 
> by reference. We use because it is more compact than base R methods. 
>
> ``` {r eval = FALSE} 
> #use ':=" to update format of timestamps
> dtc[ , detection_timestamp_utc := as.POSIXct(detection_timestamp_utc,
>                                              tz = "UTC")]
> ```  
> 
> Notice again that assignment operators _<-_ or _=_ in this code because 
> _dtc_ is updated by reference.  
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __lubridate tip:__ Use _fast\_strptime_ to set timestamps.  

> ``` {r eval = FALSE} 
> #lubridate::fast_strptime is the fastest way we know to ceorce strings to POSIX
> dtc[ , detection_timestamp_utc :=  
>          lubridate::fast_strptime(detection_timestamp_utc,
>                                   format = "%Y-%m-%d %H:%M:%OS",
>                                   tz = "UTC",
>                                   lt = FALSE)]
> ```  
> 
> Notice that we formatted the timestamps using _fast_strptime_ but also 
> used _data.table_'s _set_ operator (_:=_) to assign it to the target column.  
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


#### _**receiver_sn**_

There is no single column in the VUE export data with receiver serial number, 
so we need to extract it from the *Receiver* column.

```{r}
#make new function to extract second element from a hyphen-delimited string
get_rsn <- function(x) strsplit(x, "-")[[1]][2]

#apply get_rsn() to each record in Receiver column
dtc$receiver_sn <- sapply(dtc$Receiver, get_rsn)
```


> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip:__ Use _by_ argument in _data.table_ to update a column 
> by groups.  

> ``` {r eval = FALSE} 
> #make new column "receiver_sn"; parse from "Receiver"
> dtc[ , receiver_sn := get_rsn(Receiver), by = "Receiver"]
> ```  
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


#### _**deploy_lat**_ and _**deploy_long**_

The *Latitude* and *Longitude* values are all zero in this data set, because the  
VUE database from which these data were exported did not have any latitude or 
longitude data. We need to get them from another source. These example data only 
contain one receiver, but to demonstrate how this can be done, we will make a new 
data frame with these data and merge it. 

The code below shows a simple left join on *receiver_sn*, which assigns 
the same receiver location data to all detection records on that receiver 
without time consideration. Two limitations of this simple join are that it:

* is inadequate if any receiver is deployed at more than one location.
* includes detections that occurred before receiver deployment and after 
receiver recovery.

```{r}
#make an example receiver data frame
rcv <- data.frame(
        glatos_array = "DWM",
        station = "DWM-001", 
        deploy_lat = 45.65738, 
        deploy_long = -84.46418, 
        deploy_date_time = as.POSIXct("2011-04-11 20:30:00", tz = "UTC"),
        recover_date_time = as.POSIXct("2011-07-08 17:11:00", tz = "UTC"),
        ins_serial_no = "109924",
        stringsAsFactors = FALSE) 

#merge 
dtc <- merge(dtc, rcv, by.x = "receiver_sn", by.y = "ins_serial_no")

# take a look
head(dtc)
```

Note that new columns have been added to *dtc*, including *deploy_lat*, 
*deploy_long*, and two columns (*glatos_array* and *glatos_station*) that could 
serve as optional location grouping variables. Columns *deploy_date_time* and 
*recover_date_time* (*POSIXct* objects) are not mandatory columns, 
but are useful for removing detections that occurred before receiver deployment 
or after recovery. 


We will subset detections to omit any that occurred before deployment or after 
recovery.

```{r}
#count rows before subset
nrow(dtc)

#subset deployments between receiver deployment and recovery (omit others)
dtc <- with(dtc, dtc[detection_timestamp_utc >= deploy_date_time & 
                     detection_timestamp_utc <= recover_date_time, ])

```

> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip:__ Use _between_ to subset records by intervals.

> ``` {r eval = FALSE} 
> #subset deployments between receiver deployment and recovery (omit others) 
> dtc <- dtc[between(detection_timestamp_utc,                                 
>                    deploy_date_time, recover_date_time), ]  
> ```

> Note that the _<-_ assignment operator is used here because we are subsetting
> the _data.table_ object `dtc` and there is no assignment operator inside 
> the square brackets.  
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


We removed five rows.  

```{r}
#count rows after subset
nrow(dtc)
```

#### _**transmitter_codespace**_ and _**transmitter_id**_

There is no single column in the VUE export data with transmitter code space 
or transmitter ID code, so we need to extract them from the *Transmitter* column.

```{r}
#make a new function to extract id from Transmitter
#i.e., get third element of hyphen-delimited string
parse_tid <- function(x) strsplit(x, "-")[[1]][3]

#make a new function to extract codespace from Transmitter
#i.e., get first two elements of hyphen-delimited string
parse_tcs <- function(x) {
    #split on "-" and keep first two extracted elements
    tx <- strsplit(x, "-")[[1]][1:2]
    #re-combine and separate by "-"
    return(paste(tx[1:2], collapse = "-"))
  }

#apply parse_tcs() to Transmitter and assign to transmitter_codespace
dtc$transmitter_codespace <- sapply(dtc$Transmitter, parse_tcs)

#apply parse_tid() to Transmitter and assign to transmitter_id
dtc$transmitter_id <- sapply(dtc$Transmitter, parse_tid)

```

> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip:__ Use the _functional form_ of _:=_ to add/modify more
> than one column.

> ``` {r eval = FALSE} 
> dtc[ , `:=`(transmitter_codespace = parse_tcs(Transmitter),                    
>             transmitter_id = parse_tid(Transmitter), 
>        by = "Transmitter"]  
> ```

> See `?data.table::set` for description and examples of functional form of 
> `:=`.  
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  



#### _**sensor_value**_ and _**sensor_unit**_

Change the column names from *'Sensor Value'* and *'Sensor Unit'* to 
*sensor_value* and *sensor_unit*.

```{r}
#change column name
names(dtc)[match(c("Sensor Value", "Sensor Unit"), names(dtc))] <- 
             c("sensor_value", "sensor_unit")
```


> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip__  
> Use _setnames_ to change column names.

> ``` {r eval = FALSE} 
> setnames(dtc, c("Sensor Value", "Sensor Unit"),
>               c("sensor_value", "sensor_unit"))  
> ```
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  



```{r}
str(dtc)
```

#### _**animal_id**_

The *animal_id* column will need to come from another source, like the 
animal tagging data. We will make a new data frame with these data and 
merge it. 

The code below shows a simple left join on *transmitter_codespace* and
*transmitter_id*, which assigns the same receiver location data to all
detection records of each transmitter without time consideration. Two
limitations of this simple join are that it:

* is inadequate if any transmitter was deployed more than once.
* includes detections that occurred before transmitter deployment (animal 
  release) and after 
transmitter recovery (animal recapture).

```{r}
#make an example animal (fish) data frame
fsh <- data.frame(
        animal_id = c("1", "4", "7", "128"), 
        tag_code_space = "A69-1601",
        tag_id_code = c("439", "442", "445", "442"), 
        common_name = "Sea Lamprey", 
        release_date_time = as.POSIXct(c("2011-05-05 12:00", 
                                         "2011-05-05 12:00", 
                                         "2011-05-06 12:00", 
                                         "2011-06-08 12:00"), 
                                       tz = "UTC"),
        recapture_date_time = as.POSIXct(c(NA, "2011-05-26 15:00", NA, NA),
                                         tz = "UTC"), 
        stringsAsFactors = FALSE)

#simple left join on codespace and id
dtc <- merge(dtc, fsh, by.x = c("transmitter_codespace", "transmitter_id"), 
                       by.y = c("tag_code_space", "tag_id_code"))
          
```

Note that one tag was re-used, but we did not account for this in the above 
merge (a simple left join). So we now need to subset to omit records that 
occurred before release or after recapture. 

```{r}
#subset detections to include only those between release and recapture 
# or after release if never recaptured
dtc <- with(dtc, dtc[detection_timestamp_utc >= release_date_time & 
                     (detection_timestamp_utc <= recapture_date_time |
                       is.na(recapture_date_time)), ])
```

> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
> __data.table tip__  
> Use _between_ to query records or evaluate statements by intervals.

> ``` {r eval = FALSE} 
> dtc <- dtc[between(detection_timestamp_utc, release_date_time,
>                    recapture_date_time) | is.na(recapture_date_time), ] 
> ```
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  


We should now have a detection dataset that will meet the requirements of 
*glatos* functions.


# Receiver location data

## Requirements

*glatos* functions that accept receiver location data as input will
typically require a *data.frame* with one or more of the following
columns.  Please refer to "Data field definitions of standard GLATOS
detection export" vignette and function help (e.g.,
`?read_glatos_detections`) to identify data type and definition of
mandatory fields (link).

mandatory fields:
- deploy\_lat (link)
- deploy\_long (link)
- deploy\_date\_time (link)
- recover\_date\_time (link)

 Additionally, some functions will require at least one categorical column to 
 identify location (or group of locations). These can be specified by the user, 
 but examples of such columns in a GLATOS standard detection file are:
 
- glatos\_array (link)
- station (link)
- glatos\_project\_receiver (link)

## Examples 

### Loading GLATOS data

The *read_glatos_receivers()* function reads in receiver location data 
obtained from the GLATOS standard data export and checks that the data meet requirements 
of *glatos* functions. Data are read using *fread* in the *data.table* package, 
timestamps are formatted as class *POSIXct*.

We will get the path to the *sample_receivers.csv* (example included in the 
*glatos* package) using use *system.file()*, then read the data using 
*read_glatos_receivers()*, and view the structure of the result.

```{r echo=TRUE}
#get path to example receiver_locations file
rec_file <- system.file("extdata", 
                        "sample_receivers.csv", package = "glatos")

#read sample_receivers.csv using 'read_glatos_receivers()'
rcv <- read_glatos_receivers(rec_file)

#view structure
str(rcv)
```

The result is an object with `r ncol(rcv)` columns (including the prescribed 
columns described above) and two classes: *glatos_receivers* and *data.frame*. 
The *glatos_receivers* class label indicates that the data set was created using 
a *glatos* load function and therefore should work with any *glatos* function that 
accepts receiver data as input.




